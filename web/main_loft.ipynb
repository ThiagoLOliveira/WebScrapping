{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudscraper\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de links coletados: 760\n"
     ]
    }
   ],
   "source": [
    "options = Options()\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\")\n",
    "options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
    "options.add_experimental_option(\"useAutomationExtension\", False)\n",
    "\n",
    "# Inicialização do WebDriver\n",
    "driver = webdriver.Chrome(options=options)\n",
    "driver.maximize_window()\n",
    "\n",
    "urls = []\n",
    "base_url = \"https://loft.com.br\"\n",
    "\n",
    "# Define o número de páginas a serem percorridas\n",
    "num_paginas = 20\n",
    "\n",
    "for i in range(1, num_paginas + 1):\n",
    "    url = f\"{base_url}/venda/casas/sp/sao-paulo/tipo-casa-padrao?pagina={i}\"\n",
    "    driver.get(url)\n",
    "\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    # Coletar os links dos anúncios com a nova classe\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "    anuncios = soup.find_all(\"a\", {\"class\": \"MuiButtonBase-root MuiCardActionArea-root jss327\"})\n",
    "    for anuncio in anuncios:\n",
    "        link = anuncio[\"href\"]\n",
    "        if link.startswith(\"/\"):\n",
    "            link = base_url + link\n",
    "        urls.append(link)\n",
    "\n",
    "# Finalizando o driver\n",
    "driver.quit()\n",
    "\n",
    "print(f\"Total de links coletados: {len(urls)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "760\n"
     ]
    }
   ],
   "source": [
    "urls = list(set(urls))\n",
    "pd.DataFrame(urls).to_csv(\"urls_loft.csv\", index=False, header=False)\n",
    "print(len(urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame()\n",
    "# # Funções auxiliares\n",
    "# def deve_processar_url(url):\n",
    "#     \"\"\"Verifica se a URL deve ser processada (não começa com www)\"\"\"\n",
    "#     parsed = urlparse(url)\n",
    "#     return not parsed.netloc.startswith('www.')\n",
    "\n",
    "# def extrair_acabamentos(texto):\n",
    "#     padrao = r\"porcelanato|ceramica|cerâmica|cerâmico|cerâmicos|gesso|antiderrapante|planejado|tábua\"\n",
    "#     if isinstance(texto, str):\n",
    "#         encontrados = re.findall(padrao, texto.lower())\n",
    "#         return \", \".join(set(encontrados)) if encontrados else None\n",
    "#     return None\n",
    "\n",
    "# def extrair_tipo_imovel(texto):\n",
    "#     padrao = r\"casa|apartamento|sobrado|fazenda\"\n",
    "#     if isinstance(texto, str):\n",
    "#         encontrados = re.findall(padrao, texto.lower())\n",
    "#         return \", \".join(set(encontrados)) if encontrados else None\n",
    "#     return None\n",
    "\n",
    "# scraper = cloudscraper.create_scraper()\n",
    "# # Remove duplicatas\n",
    "# urls = list(set(urls))  \n",
    "\n",
    "# for url in urls:\n",
    "#     time.sleep(1)\n",
    "#     #Pula as urls que iniciam com www\n",
    "#     try:\n",
    "#         if not deve_processar_url(url):\n",
    "#             print(f\"Pulando URL não permitida: {url}\")\n",
    "#             continue\n",
    "        \n",
    "#         response = scraper.get(url, headers={\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\"})\n",
    "\n",
    "#         if response.status_code == 200:\n",
    "#             soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "#             dados_imovel = {\"Link\": url}\n",
    "\n",
    "#             # Busca Localização\n",
    "#             location = soup.find(\"div\", {\"id\": \"location\"})\n",
    "#             if location:\n",
    "#                 endereco_texto = location.get_text(\" \", strip=True)\n",
    "#                 cep = re.findall(r\"\\d{5}-?\\d{3}\", endereco_texto)\n",
    "#                 cep = cep[-1] if cep else None\n",
    "#                 endereco_limpo = endereco_texto.replace(\"Localização\", \"\")\\\n",
    "#                                             .replace(\"Exibir no mapa\", \"\")\\\n",
    "#                                             .replace(cep, \"\")\\\n",
    "#                                             .strip()\n",
    "#                 endereco_limpo = re.sub(r'[,\\s-]+$', '', endereco_limpo).strip()\n",
    "#                 dados_imovel.update({\n",
    "#                     \"Endereço\": endereco_limpo,\n",
    "#                     \"CEP\": cep\n",
    "#                 })\n",
    "\n",
    "#             # Busca Valor\n",
    "#             div_valor_mae = soup.find(\"div\", {\"id\": \"price-box-container\"}) \n",
    "#             if div_valor_mae:\n",
    "#                 div_valor = div_valor_mae.find(\"div\", {\"class\": \"olx-d-flex olx-fd-column\"}) if div_valor_mae else None\n",
    "#                 if div_valor:\n",
    "#                     valor_texto = div_valor.get_text(strip=True)\n",
    "#                     valor_limpo = re.sub(r'[^\\d,]', '', valor_texto).replace(',', '.').strip()\n",
    "#                     dados_imovel[\"Valor Imóvel\"] = valor_limpo if valor_limpo else None\n",
    "\n",
    "\n",
    "#             # Busca características do imóvel\n",
    "#             div_principal = soup.find(\"div\", {\"class\": \"ad__sc-wuor06-0 hfcCRQ\"})\n",
    "#             if div_principal:\n",
    "#                 divs_caracteristicas = div_principal.find_all(\"div\", recursive=False)\n",
    "                \n",
    "#                 for div in divs_caracteristicas:\n",
    "#                     texto = div.get_text(strip=True)\n",
    "#                     if \"Área construída\" in texto:\n",
    "#                         dados_imovel[\"Área construída\"] = re.search(r\"(\\d+)\", texto).group(1) if re.search(r\"(\\d+)\", texto) else None\n",
    "#                     elif \"Quartos\" in texto:\n",
    "#                         dados_imovel[\"Quartos\"] = texto.replace(\"Quartos\", \"\").strip()\n",
    "#                     elif \"Banheiros\" in texto:\n",
    "#                         dados_imovel[\"Banheiros\"] = texto.replace(\"Banheiros\", \"\").strip()\n",
    "#                     elif \"Vagas na garagem\" in texto:\n",
    "#                         dados_imovel[\"Vagas na garagem\"] = texto.replace(\"Vagas na garagem\", \"\").strip()\n",
    "\n",
    "#             # Descrição do imóvel\n",
    "#             descricao = soup.find(\"div\", {\"id\": \"description-title\"})\n",
    "#             descricao_texto = descricao.get_text(\" \", strip=True) if descricao else None\n",
    "#             dados_imovel[\"Descrição\"] = descricao_texto\n",
    "\n",
    "#             # Busca detalhes adicionais\n",
    "#             if descricao:\n",
    "#                 desc_texto = descricao.get_text(\" \", strip=True)\n",
    "#                 dados_imovel[\"Acabamentos\"] = extrair_acabamentos(desc_texto)\n",
    "#                 dados_imovel[\"Tipo Imóvel\"] = extrair_tipo_imovel(desc_texto)\n",
    "\n",
    "#             df = pd.concat([df, pd.DataFrame([dados_imovel])], ignore_index=True)\n",
    "#             print(dados_imovel)\n",
    "#     except Exception as e:\n",
    "#         print(f\"Erro ao processar a URL {url}: {e}\")\n",
    "#         continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_excel(\"databases/imoveis.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
